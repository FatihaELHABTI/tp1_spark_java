
# TP Spark Java â€” Analyse de donnÃ©es (Ventes & Logs Apache)

## ğŸ¯ Objectif du Projet
Ce projet permet de pratiquer **Apache Spark en Java** via deux exercices :

- **Exercice 1 : Analyse des ventes**  
  Traitement dâ€™un fichier `ventes.txt` pour calculer les ventes par ville et par annÃ©e.

- **Exercice 2 : Analyse de logs Apache**  
  Extraction dâ€™informations depuis `access.log` (IP, ressource, code HTTP, erreurs, statistiquesâ€¦).

Le tout fonctionne **en local** ou via un **cluster Hadoop + Spark DockerisÃ©**.

---

# ğŸ› ï¸ 1. Structure du Projet

```
tp1-spark-java/
â”‚â”€â”€ src/main/java/ma/enset/
â”‚   â”œâ”€â”€ Exercice1.java
â”‚   â””â”€â”€ Exercice2.java
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ ventes.txt
â”‚   â””â”€â”€ access.log
â”‚â”€â”€ pom.xml
â”‚â”€â”€ docker-compose.yaml
â”‚â”€â”€ config/ (fichiers core-site, yarn-siteâ€¦)
```

---

# ğŸš€ 2. ExÃ©cution avec Spark local ou Hadoop + Docker

### âœ… Mode local
Spark lit les fichiers depuis `data/`.

```java
JavaRDD<String> lines = sc.textFile("data/ventes.txt");
```

### âœ… Mode cluster Hadoop/Spark
Spark lit depuis **HDFS** :

```java
JavaRDD<String> lines = sc.textFile("hdfs://namenode:8020/data/ventes.txt");
```

---

# ğŸ“¦ 3. Configuration Maven (pom.xml)

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.13</artifactId>
    <version>${spark.version}</version>
</dependency>
```

- JDK : **17**
- Spark Core : **4.0.1**
- Mode Standalone ou cluster Hadoop/Spark

---

# ğŸ“Š 4. Exercice 1 â€” Analyse des ventes

## ğŸ“ 4.1. Chargement des donnÃ©es

Chaque ligne du fichier `ventes.txt` :

```
2024-05-10 Paris Ordinateur 1200.50
```

DÃ©coupage en Spark :

```java
String[] parts = line.split(" ");
String ville = parts[1];
double prix = Double.parseDouble(parts[3]);
```

---

## ğŸ“ 4.2. Total des ventes par ville

```java
JavaPairRDD<String, Double> ventesParVille =
    lines.mapToPair(line -> new Tuple2<>(ville, prix));

JavaPairRDD<String, Double> totalParVille =
    ventesParVille.reduceByKey(Double::sum);
```

Sortie typique :

```
Ville: Paris, Total: 2725.99
Ville: Lyon, Total: 2775.00
Ville: Marseille, Total: 1580.00
```

---

## ğŸ“… 4.3. Total par ville et par annÃ©e

```java
String annee = parts[0].substring(0, 4);
return new Tuple2<>(new Tuple2<>(annee, ville), prix);
```

---

# ğŸ“‘ 5. Exercice 2 â€” Analyse de Logs Apache

## ğŸ§© 5.1. Format du fichier `access.log`

Une ligne exemple :

```
127.0.0.1 - - [10/Oct/2025:09:15:32 +0000] "GET /index.html HTTP/1.1" 200 1024
```

### ğŸ“Œ Extraction via Regex

```java
private static final String LOG_REGEX =
  "^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] \"(\S+) (\S+) (\S+)\" (\d{3}) (\d+)";
```

Classe reprÃ©sentant une entrÃ©e :

```java
public static class LogEntry implements Serializable {
    String ip, dateTime, method, resource;
    int httpCode;
    long responseSize;
}
```

---

## ğŸ“Š 5.2. Statistiques globales

### âœ” Nombre total de requÃªtes  
```java
long totalRequests = parsedLogs.count();
```

### âœ” Nombre dâ€™erreurs HTTP (>=400)

```java
parsedLogs.filter(log -> log.httpCode >= 400).count();
```

---

## ğŸ¥‡ 5.3. Top 5 adresses IP

```java
parsedLogs.mapToPair(log -> new Tuple2<>(log.ip, 1))
          .reduceByKey(Integer::sum)
```

---

## ğŸ¥‡ 5.4. Top 5 ressources les plus consultÃ©es

```java
parsedLogs.mapToPair(log -> new Tuple2<>(log.resource, 1))
```

---

## ğŸ“ˆ 5.5. RÃ©partition des codes HTTP

```java
parsedLogs.mapToPair(log -> new Tuple2<>(log.httpCode, 1))
```

---

# ğŸ³ 6. Cluster Hadoop + Spark (docker-compose.yaml)

Le projet inclut :

- **Hadoop NameNode / DataNode**
- **YARN ResourceManager / NodeManager**
- **Spark Master / Worker**
- RÃ©seau : `spark-network`

L'application Spark se connecte automatiquement Ã  :

```
hdfs://namenode:8020
```

---

# âš™ï¸ 7. Fichiers de configuration Hadoop (config/)

Exemples :

### core-site.xml
```
fs.defaultFS=hdfs://namenode
```

### hdfs-site.xml
```
dfs.replication=3
```

### yarn-site.xml
```
yarn.resourcemanager.hostname=resourcemanager
```

---

# ğŸ“ 8. Jeux de donnÃ©es

## âœ” ventes.txt
Contient : date, ville, produit, prix.

## âœ” access.log
Contient : IP, date, mÃ©thode HTTP, ressource, code HTTP, taille rÃ©ponse.

---

# ğŸ 9. Conclusion

Ce TP met en pratique :

- Spark Core (RDD)
- Transformations & actions (map, reduceByKey, filterâ€¦)
- Analyse de fichiers structurÃ©s (ventes) et semiâ€‘structurÃ©s (logs)
- ExÃ©cution Spark standalone ou cluster Hadoop/Spark
- IntÃ©gration HDFS pour les datasets

Ce README permet de comprendre clairement la logique du code, les concepts Spark utilisÃ©s et la configuration complÃ¨te du cluster.

---

# ğŸ‘¤ Auteur
TP rÃ©alisÃ© par **Fatiha EL HABTI** dans le cadre du module **Big Data - Spark**.
